---
title: Migration mechanism of heterogeneous memory pages using a two-way Hash chain list

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - Yanfei Ji
  - Tianma Shen
  - Songwen Pei


# Author notes (optional)
#author_notes:
#  - 'Equal contribution'
#  - 'Equal contribution'

date: '2019-08-01T00:00:00Z'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2019-08-01T00:00:00Z'

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ['2']

# Publication name and optional abbreviated publication name.
publication: "*SCIENTIA SINICA Informationis*"
publication_short: ""

#abstract:  The invention relates to an adversarial optimization method for the training process of generative adversarial network. According to the adversarial optimization method for the training process of generative adversarial network, the optimal transmission problem is transformed into solving the elliptic Monge-Ampere partial differential equation (MAPDE) in the generator G. To solve MAPDE of n (n>3) dimensions, the Neumann boundary conditions are improved and the discretization of MAPDE is extended to obtain the optimal mapping between a generator and a discriminator, which constitutes the adversarial network MAGAN. In the process of training the defence network, by overcoming the loss function of the optimal mapping, the defence network can obtain a maximum distance between the two measurements and obtain filtered security samples. The effective attack method of GANs is successfully established, with the precision improved by 5.3%. In addition, the MAGAN can be stably trained without adjusting hyper-parameters, so that the accuracy of target classification and recognition system for unmanned vehicle can be well improved.
  
# Summary. An optional shortened abstract.
#summary: In this paper, we propose a novel learning-based image coding system using transformer structures. Our context model codes latent representations in a channel-first order, followed by a 2D zigzag spatial order. Along with transformer structures, such context model more effectively extracts contextual information for better entropy coding. Further, we propose a transformer-based latent residual cross-attention prediction (LRCP) module to reduce the quantization error.  Compared to existing learned image compression approaches and traditional image compression methods, our proposed model achieved significantly better perceptual quality and RD performance. 

tags: []

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
#image:
#  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)'
#  focal_point: ''
#  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---

