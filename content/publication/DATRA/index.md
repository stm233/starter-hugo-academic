---
title: DATRA, A power-aware dynamic adaptive threshold routing algorithm for dragonfly network-on-chip topology

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - Songwen Pei
  - Jihong Yuan
  - Yanfei Ji
  - Tianma Shen

# Author notes (optional)
#author_notes:
#  - 'Equal contribution'
#  - 'Equal contribution'

date: '2019-06-01T00:00:00Z'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2019-06-01T00:00:00Z'

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ['1']

# Publication name and optional abbreviated publication name.
publication: In *IEEE Intl Conf on Parallel \& Distributed Processing with Applications, Big Data \& Cloud Computing, Sustainable Computing \& Communications, Social Computing \& Networking*
publication_short: ""

abstract:  Due to the issues of significant power consumption and extra hops from source to destination in the Dragonfly topology of network-on-chip (NoC) system, it becomes increasingly important to develop a power-aware routing algorithm for the Dragonfly topology. We propose a dynamic adaptive threshold routing algorithm (DATRA) by applying dynamic adaptive threshold and its own dynamic threshold step to balance the load of network on chip and achieve lower power-delay-product. In order to evaluate DATRA, we propose an evaluation model by extending terms Ptotal, Lavg, VAR(load), and Havg, etc. The experimental results verified that DATRA can respectively save 6.53% (n=4) and 5.93% (n=8) on power-delay product on average by comparing to the UGAL-LVC-H, which implies that DATRA is a promising alternative strategy of routing algorithm for the Dragonfly topology.

# Summary. An optional shortened abstract.
#summary: In this paper, we propose a novel learning-based image coding system using transformer structures. Our context model codes latent representations in a channel-first order, followed by a 2D zigzag spatial order. Along with transformer structures, such context model more effectively extracts contextual information for better entropy coding. Further, we propose a transformer-based latent residual cross-attention prediction (LRCP) module to reduce the quantization error.  Compared to existing learned image compression approaches and traditional image compression methods, our proposed model achieved significantly better perceptual quality and RD performance. 

tags: []

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
#image:
#  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)'
#  focal_point: ''
#  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---

